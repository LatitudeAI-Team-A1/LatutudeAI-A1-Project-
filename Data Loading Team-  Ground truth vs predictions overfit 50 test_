{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2X0au3kegkbL","outputId":"2cbf0383-246d-4003-a6f1-d586bf9a92ef"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n","  | |_| | '_ \\/ _` / _` |  _/ -_)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"]}],"source":["# =========================\n","# Overfit 50 images — Mask R-CNN (BDD classes kept separate) + W&B logging\n","# The core logic is correct: ONLY BDD_IDS (Things) are processed.\n","# This code version includes explicit mask loading (Image.convert('L')) and max_area_ratio checks.\n","# =========================\n","!pip install wandb -q\n","import wandb\n","wandb.login()\n","\n","import os\n","import time\n","import torch\n","import numpy as np\n","from PIL import Image\n","import torchvision\n","import torchvision.transforms as T\n","from torch.utils.data import Dataset, DataLoader\n","from scipy.ndimage import label as cc_label\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from google.colab import drive\n","\n","# -------------------------\n","# Paths (your Drive layout)\n","# -------------------------\n","drive.mount('/content/drive', force_remount=True)\n","\n","BDD_ROOT = \"/content/drive/MyDrive/Latitude_AI_Team/data\"\n","IMG_DIR = f\"{BDD_ROOT}/bdd100k_images_10k/10k/train\"\n","MASK_DIR = f\"{BDD_ROOT}/bdd100k_segmentations/bdd100k_seg_maps_2/labels/train\"\n","\n","print(\"Images path exists:\", os.path.exists(IMG_DIR))\n","print(\"Masks path exists :\", os.path.exists(MASK_DIR))\n","print(\"Num images:\", len(os.listdir(IMG_DIR)))\n","print(\"Num masks :\", len(os.listdir(MASK_DIR)))\n","\n","# -------------------------\n","# Device + Hyperparams\n","# -------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", device)\n","\n","SUBSET = 50\n","EPOCHS = 20\n","BATCH_SIZE = 2\n","LR = 1e-4\n","\n","# *** CRITICAL: ONLY BDD INSTANCE (THING) CLASSES ARE LISTED HERE ***\n","# This guarantees that IDs like 8 (Vegetation/Tree) and 10 (Sky) are ignored.\n","BDD_IDS = [\n","    6,  # Traffic Light\n","    7,  # Traffic Sign\n","    11, # Person\n","    13, # Car\n","    12, # Rider\n","    14, # Truck\n","    15, # Bus\n","    17, # Motorcycle\n","    18  # Bicycle\n","]\n","NUM_FOREGROUND = len(BDD_IDS)\n","NUM_CLASSES = 1 + NUM_FOREGROUND\n","\n","ID_TO_LABEL = {bdd_id: i+1 for i, bdd_id in enumerate(BDD_IDS)}\n","print(\"Mapping BDD IDs -> labels:\", ID_TO_LABEL)\n","\n","# -------------------------\n","# Dataset (semantic -> instances)\n","# -------------------------\n","class BDDOverfitDataset(Dataset):\n","    def __init__(self, img_dir, mask_dir, subset=50, transforms=None, min_area=200, max_area_ratio=0.5):\n","        self.img_dir = img_dir\n","        self.mask_dir = mask_dir\n","        self.transforms = transforms\n","        self.min_area = min_area\n","        self.max_area_ratio = max_area_ratio\n","\n","        imgs = sorted([f for f in os.listdir(img_dir) if f.lower().endswith(('.jpg', '.png'))])\n","        masks = sorted([f for f in os.listdir(mask_dir) if f.lower().endswith(('.png', '.jpg'))])\n","\n","        # Align by basename (This logic is CONFIRMED correct)\n","        img_map = {os.path.splitext(f)[0]: f for f in imgs}\n","        mask_map = {os.path.splitext(f)[0].replace(\"_train_id\", \"\"): f for f in masks}\n","\n","        common = sorted(set(img_map.keys()).intersection(set(mask_map.keys())))\n","        if len(common) == 0:\n","            raise RuntimeError(\"No matching image/mask basenames found. Check directories and filenames.\")\n","\n","        # Keep only subset\n","        common = common[:subset]\n","        self.images = [img_map[k] for k in common]\n","        self.masks = [mask_map[k] for k in common]\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.images[idx])\n","        mask_path = os.path.join(self.mask_dir, self.masks[idx])\n","\n","        img = Image.open(img_path).convert(\"RGB\")\n","\n","        # *** CONFIRMED FIX: Explicitly load mask as single channel (L) ***\n","        mask_pil = Image.open(mask_path).convert('L')\n","        mask_np = np.array(mask_pil)\n","\n","        # Calculate max allowed area based on image dimensions\n","        H, W = mask_np.shape\n","        max_allowed_area = H * W * self.max_area_ratio\n","\n","        instance_masks = []\n","        labels = []\n","        boxes = []\n","\n","        # This loop only iterates over the desired THING IDs\n","        for bdd_id in BDD_IDS:\n","\n","            class_mask = (mask_np == bdd_id).astype(np.uint8)\n","\n","            if class_mask.sum() == 0:\n","                continue\n","\n","            inst_map, ninst = cc_label(class_mask)\n","            for inst_id in range(1, ninst + 1):\n","                inst_mask = (inst_map == inst_id)\n","                current_area = inst_mask.sum()\n","\n","                # Filter 1: Min Area\n","                if current_area < self.min_area:\n","                    continue\n","\n","                # Filter 2: Max Area (Robustness check)\n","                if current_area > max_allowed_area:\n","                    continue\n","\n","                ys, xs = np.where(inst_mask)\n","                xmin, ymin, xmax, ymax = xs.min(), ys.min(), xs.max(), ys.max()\n","                instance_masks.append(inst_mask.astype(np.uint8))\n","                labels.append(ID_TO_LABEL[bdd_id])\n","                boxes.append([xmin, ymin, xmax, ymax])\n","\n","        if len(instance_masks) == 0:\n","            return None\n","\n","        masks_tensor = torch.as_tensor(np.stack(instance_masks), dtype=torch.uint8)\n","        labels_tensor = torch.as_tensor(labels, dtype=torch.int64)\n","        boxes_tensor = torch.as_tensor(boxes, dtype=torch.float32)\n","        image_tensor = T.ToTensor()(img)\n","\n","        target = {\n","            \"boxes\": boxes_tensor,\n","            \"labels\": labels_tensor,\n","            \"masks\": masks_tensor,\n","            \"image_id\": torch.tensor([idx], dtype=torch.int64)\n","        }\n","\n","        if self.transforms:\n","            image_tensor = self.transforms(image_tensor)\n","\n","        return image_tensor, target\n","\n","# collate function (CONFIRMED correct)\n","def collate_fn_filter(batch):\n","    batch = [b for b in batch if b is not None]\n","    if len(batch) == 0:\n","        return tuple()\n","    return tuple(zip(*batch))\n","\n","# -------------------------\n","# Dataset + loader\n","# -------------------------\n","dataset = BDDOverfitDataset(IMG_DIR, MASK_DIR, subset=SUBSET, transforms=None, min_area=200, max_area_ratio=0.5)\n","loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=collate_fn_filter)\n","print(\"Dataset length after subset:\", len(dataset))\n","\n","# -------------------------\n","# Model\n","# -------------------------\n","def get_model(num_classes):\n","    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n","    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","    model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(in_features_mask, 256, num_classes)\n","    return model\n","\n","model = get_model(NUM_CLASSES).to(device)\n","optimizer = torch.optim.Adam([p for p in model.parameters() if p.requires_grad], lr=LR)\n","\n","# -------------------------\n","# W&B init\n","# -------------------------\n","wandb.init(project=\"bdd100k-overfit-50-allclasses\", name=\"overfit-50-allclasses-final-fix\", config={\n","    \"subset\": SUBSET, \"epochs\": EPOCHS, \"batch_size\": BATCH_SIZE, \"lr\": LR, \"num_classes\": NUM_CLASSES\n","})\n","\n","# -------------------------\n","# Visualization helper\n","# -------------------------\n","def make_vis_figure(img_tensor, target, prediction=None, conf_th=0.5):\n","    img_np = (img_tensor.permute(1,2,0).cpu().numpy() * 255).astype(np.uint8)\n","    fig, ax = plt.subplots(1,1, figsize=(10,6))\n","    ax.imshow(img_np)\n","    ax.axis('off')\n","\n","    # GT masks in green\n","    if target is not None:\n","        masks = target[\"masks\"].cpu().numpy()\n","        boxes = target[\"boxes\"].cpu().numpy()\n","        labels = target[\"labels\"].cpu().numpy()\n","        for k in range(masks.shape[0]):\n","            mask = masks[k]\n","            ax.imshow(np.ma.masked_where(mask==0, mask), cmap='Greens', alpha=0.4)\n","            x1,y1,x2,y2 = boxes[k]\n","            ax.add_patch(patches.Rectangle((x1,y1), x2-x1, y2-y1, edgecolor='lime', fill=False, linewidth=2))\n","            ax.text(x1, y1-5, f\"GT:{labels[k]}\", color='lime', fontsize=8, weight='bold')\n","\n","    # Prediction masks in red\n","    if prediction is not None:\n","        pred_masks = prediction.get(\"masks\", None)\n","        pred_boxes = prediction.get(\"boxes\", None)\n","        pred_scores = prediction.get(\"scores\", torch.tensor([])).cpu().numpy()\n","        keep = pred_scores >= conf_th\n","\n","        if pred_masks is not None and keep.sum() > 0:\n","            pred_masks = pred_masks.cpu().numpy()[keep,0]\n","            for mask in pred_masks:\n","                ax.imshow(np.ma.masked_where(mask<0.5, mask), cmap='Reds', alpha=0.4)\n","\n","        if pred_boxes is not None and keep.sum() > 0:\n","            pred_boxes = pred_boxes.cpu().numpy()[keep]\n","            for b in pred_boxes:\n","                x1,y1,x2,y2 = b\n","                ax.add_patch(patches.Rectangle((x1,y1), x2-x1, y2-y1, edgecolor='red', fill=False, linewidth=2))\n","\n","    return fig\n","\n","# -------------------------\n","# Training loop\n","# -------------------------\n","best_loss = float('inf')\n","global_step = 0\n","model.train()\n","\n","for epoch in range(1, EPOCHS+1):\n","    epoch_loss = 0.0\n","    batches = 0\n","    start = time.time()\n","    for batch in loader:\n","        if len(batch) == 0:\n","            continue\n","        images, targets = batch\n","        images = [img.to(device) for img in images]\n","        targets = [{k: v.to(device) for k,v in t.items()} for t in targets]\n","\n","        loss_dict = model(images, targets)\n","        losses = sum(loss for loss in loss_dict.values())\n","\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","\n","        batch_loss = float(losses.item())\n","        epoch_loss += batch_loss\n","        batches += 1\n","        global_step += 1\n","        wandb.log({\"batch_loss\": batch_loss, \"epoch\": epoch, \"step\": global_step})\n","\n","    avg_loss = epoch_loss / batches if batches>0 else epoch_loss\n","    print(f\"Epoch {epoch}/{EPOCHS} — avg_loss: {avg_loss:.4f} — time: {time.time()-start:.1f}s\")\n","    wandb.log({\"epoch_loss\": avg_loss, \"epoch\": epoch})\n","\n","    # Save checkpoint if improved\n","    if avg_loss < best_loss:\n","        best_loss = avg_loss\n","        ckpt_path = \"best_overfit_50.pth\"\n","        torch.save(model.state_dict(), ckpt_path)\n","        wandb.save(ckpt_path)\n","        print(\"  Saved best checkpoint:\", ckpt_path)\n","\n","    # Visualization\n","    if epoch % 5 == 0 or epoch == EPOCHS:\n","        model.eval()\n","        for data in loader:\n","            if len(data) == 0:\n","                continue\n","            imgs, tgts = data\n","            break\n","        with torch.no_grad():\n","            img0 = imgs[0]\n","            pred = model([img0.to(device)])[0]\n","        fig = make_vis_figure(img0, tgts[0], pred, conf_th=0.5)\n","        wandb.log({f\"viz_epoch_{epoch}\": wandb.Image(fig)})\n","        plt.close(fig)\n","        model.train()\n","\n","print(\"Training complete. Best avg loss:\", best_loss)\n","wandb.finish()\n","torch.save(model.state_dict(), \"final_overfit_50.pth\")\n","print(\"Saved final_overfit_50.pth\")"]},{"cell_type":"code","source":["# =========================\n","# Overfit 50 images — Mask R-CNN (BDD classes kept separate) + W&B logging\n","# Enhanced with 15-image visualization after training\n","# =========================\n","!pip install wandb -q\n","import wandb\n","wandb.login()\n","\n","import os\n","import time\n","import torch\n","import numpy as np\n","from PIL import Image\n","import torchvision\n","import torchvision.transforms as T\n","from torch.utils.data import Dataset, DataLoader\n","from scipy.ndimage import label as cc_label\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from google.colab import drive\n","\n","# -------------------------\n","# Paths (your Drive layout)\n","# -------------------------\n","drive.mount('/content/drive', force_remount=True)\n","\n","BDD_ROOT = \"/content/drive/MyDrive/Latitude_AI_Team/data\"\n","IMG_DIR = f\"{BDD_ROOT}/bdd100k_images_10k/10k/train\"\n","MASK_DIR = f\"{BDD_ROOT}/bdd100k_segmentations/bdd100k_seg_maps_2/labels/train\"\n","\n","print(\"Images path exists:\", os.path.exists(IMG_DIR))\n","print(\"Masks path exists :\", os.path.exists(MASK_DIR))\n","print(\"Num images:\", len(os.listdir(IMG_DIR)))\n","print(\"Num masks :\", len(os.listdir(MASK_DIR)))\n","\n","# -------------------------\n","# Device + Hyperparams\n","# -------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", device)\n","\n","SUBSET = 50\n","EPOCHS = 20\n","BATCH_SIZE = 2\n","LR = 1e-4\n","\n","# *** CRITICAL: ONLY BDD INSTANCE (THING) CLASSES ARE LISTED HERE ***\n","BDD_IDS = [\n","    6,  # Traffic Light\n","    7,  # Traffic Sign\n","    11, # Person\n","    13, # Car\n","    12, # Rider\n","    14, # Truck\n","    15, # Bus\n","    17, # Motorcycle\n","    18  # Bicycle\n","]\n","NUM_FOREGROUND = len(BDD_IDS)\n","NUM_CLASSES = 1 + NUM_FOREGROUND\n","\n","ID_TO_LABEL = {bdd_id: i+1 for i, bdd_id in enumerate(BDD_IDS)}\n","LABEL_TO_NAME = {\n","    1: \"Traffic Light\", 2: \"Traffic Sign\", 3: \"Person\", 4: \"Car\",\n","    5: \"Rider\", 6: \"Truck\", 7: \"Bus\", 8: \"Motorcycle\", 9: \"Bicycle\"\n","}\n","print(\"Mapping BDD IDs -> labels:\", ID_TO_LABEL)\n","\n","# -------------------------\n","# Dataset (semantic -> instances)\n","# -------------------------\n","class BDDOverfitDataset(Dataset):\n","    def __init__(self, img_dir, mask_dir, subset=50, transforms=None, min_area=200, max_area_ratio=0.5):\n","        self.img_dir = img_dir\n","        self.mask_dir = mask_dir\n","        self.transforms = transforms\n","        self.min_area = min_area\n","        self.max_area_ratio = max_area_ratio\n","\n","        imgs = sorted([f for f in os.listdir(img_dir) if f.lower().endswith(('.jpg', '.png'))])\n","        masks = sorted([f for f in os.listdir(mask_dir) if f.lower().endswith(('.png', '.jpg'))])\n","\n","        img_map = {os.path.splitext(f)[0]: f for f in imgs}\n","        mask_map = {os.path.splitext(f)[0].replace(\"_train_id\", \"\"): f for f in masks}\n","\n","        common = sorted(set(img_map.keys()).intersection(set(mask_map.keys())))\n","        if len(common) == 0:\n","            raise RuntimeError(\"No matching image/mask basenames found.\")\n","\n","        common = common[:subset]\n","        self.images = [img_map[k] for k in common]\n","        self.masks = [mask_map[k] for k in common]\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.images[idx])\n","        mask_path = os.path.join(self.mask_dir, self.masks[idx])\n","\n","        img = Image.open(img_path).convert(\"RGB\")\n","        mask_pil = Image.open(mask_path).convert('L')\n","        mask_np = np.array(mask_pil)\n","\n","        H, W = mask_np.shape\n","        max_allowed_area = H * W * self.max_area_ratio\n","\n","        instance_masks = []\n","        labels = []\n","        boxes = []\n","\n","        for bdd_id in BDD_IDS:\n","            class_mask = (mask_np == bdd_id).astype(np.uint8)\n","            if class_mask.sum() == 0:\n","                continue\n","\n","            inst_map, ninst = cc_label(class_mask)\n","            for inst_id in range(1, ninst + 1):\n","                inst_mask = (inst_map == inst_id)\n","                current_area = inst_mask.sum()\n","\n","                if current_area < self.min_area:\n","                    continue\n","                if current_area > max_allowed_area:\n","                    continue\n","\n","                ys, xs = np.where(inst_mask)\n","                xmin, ymin, xmax, ymax = xs.min(), ys.min(), xs.max(), ys.max()\n","                instance_masks.append(inst_mask.astype(np.uint8))\n","                labels.append(ID_TO_LABEL[bdd_id])\n","                boxes.append([xmin, ymin, xmax, ymax])\n","\n","        if len(instance_masks) == 0:\n","            return None\n","\n","        masks_tensor = torch.as_tensor(np.stack(instance_masks), dtype=torch.uint8)\n","        labels_tensor = torch.as_tensor(labels, dtype=torch.int64)\n","        boxes_tensor = torch.as_tensor(boxes, dtype=torch.float32)\n","        image_tensor = T.ToTensor()(img)\n","\n","        target = {\n","            \"boxes\": boxes_tensor,\n","            \"labels\": labels_tensor,\n","            \"masks\": masks_tensor,\n","            \"image_id\": torch.tensor([idx], dtype=torch.int64)\n","        }\n","\n","        if self.transforms:\n","            image_tensor = self.transforms(image_tensor)\n","\n","        return image_tensor, target\n","\n","def collate_fn_filter(batch):\n","    batch = [b for b in batch if b is not None]\n","    if len(batch) == 0:\n","        return tuple()\n","    return tuple(zip(*batch))\n","\n","# -------------------------\n","# Dataset + loader\n","# -------------------------\n","dataset = BDDOverfitDataset(IMG_DIR, MASK_DIR, subset=SUBSET, transforms=None, min_area=200, max_area_ratio=0.5)\n","loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=collate_fn_filter)\n","print(\"Dataset length after subset:\", len(dataset))\n","\n","# -------------------------\n","# Model\n","# -------------------------\n","def get_model(num_classes):\n","    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n","    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","    model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(in_features_mask, 256, num_classes)\n","    return model\n","\n","model = get_model(NUM_CLASSES).to(device)\n","optimizer = torch.optim.Adam([p for p in model.parameters() if p.requires_grad], lr=LR)\n","\n","# -------------------------\n","# W&B init\n","# -------------------------\n","wandb.init(project=\"bdd100k-overfit-50-allclasses\", name=\"overfit-50-allclasses-final-fix\", config={\n","    \"subset\": SUBSET, \"epochs\": EPOCHS, \"batch_size\": BATCH_SIZE, \"lr\": LR, \"num_classes\": NUM_CLASSES\n","})\n","\n","# -------------------------\n","# Visualization helper\n","# -------------------------\n","def make_vis_figure(img_tensor, target, prediction=None, conf_th=0.5, img_idx=None):\n","    img_np = (img_tensor.permute(1,2,0).cpu().numpy() * 255).astype(np.uint8)\n","    fig, ax = plt.subplots(1,1, figsize=(12,8))\n","    ax.imshow(img_np)\n","    ax.axis('off')\n","\n","    title = f\"Image {img_idx}\" if img_idx is not None else \"Prediction\"\n","    ax.set_title(title, fontsize=14, weight='bold')\n","\n","    # GT masks in green\n","    if target is not None:\n","        masks = target[\"masks\"].cpu().numpy()\n","        boxes = target[\"boxes\"].cpu().numpy()\n","        labels = target[\"labels\"].cpu().numpy()\n","        for k in range(masks.shape[0]):\n","            mask = masks[k]\n","            ax.imshow(np.ma.masked_where(mask==0, mask), cmap='Greens', alpha=0.4)\n","            x1,y1,x2,y2 = boxes[k]\n","            ax.add_patch(patches.Rectangle((x1,y1), x2-x1, y2-y1, edgecolor='lime', fill=False, linewidth=2))\n","            label_name = LABEL_TO_NAME.get(labels[k].item(), str(labels[k].item()))\n","            ax.text(x1, y1-5, f\"GT:{label_name}\", color='lime', fontsize=10, weight='bold',\n","                    bbox=dict(boxstyle='round,pad=0.3', facecolor='black', alpha=0.5))\n","\n","    # Prediction masks in red\n","    if prediction is not None:\n","        pred_masks = prediction.get(\"masks\", None)\n","        pred_boxes = prediction.get(\"boxes\", None)\n","        pred_labels = prediction.get(\"labels\", torch.tensor([])).cpu().numpy()\n","        pred_scores = prediction.get(\"scores\", torch.tensor([])).cpu().numpy()\n","        keep = pred_scores >= conf_th\n","\n","        if pred_masks is not None and keep.sum() > 0:\n","            pred_masks = pred_masks.cpu().numpy()[keep,0]\n","            pred_labels_filtered = pred_labels[keep]\n","            pred_scores_filtered = pred_scores[keep]\n","\n","            for i, mask in enumerate(pred_masks):\n","                ax.imshow(np.ma.masked_where(mask<0.5, mask), cmap='Reds', alpha=0.4)\n","\n","        if pred_boxes is not None and keep.sum() > 0:\n","            pred_boxes = pred_boxes.cpu().numpy()[keep]\n","            for i, b in enumerate(pred_boxes):\n","                x1,y1,x2,y2 = b\n","                ax.add_patch(patches.Rectangle((x1,y1), x2-x1, y2-y1, edgecolor='red', fill=False, linewidth=2))\n","                label_name = LABEL_TO_NAME.get(pred_labels_filtered[i], str(pred_labels_filtered[i]))\n","                score = pred_scores_filtered[i]\n","                ax.text(x1, y2+15, f\"Pred:{label_name} ({score:.2f})\", color='red', fontsize=10, weight='bold',\n","                        bbox=dict(boxstyle='round,pad=0.3', facecolor='black', alpha=0.5))\n","\n","    plt.tight_layout()\n","    return fig\n","\n","# -------------------------\n","# Training loop\n","# -------------------------\n","best_loss = float('inf')\n","global_step = 0\n","model.train()\n","\n","for epoch in range(1, EPOCHS+1):\n","    epoch_loss = 0.0\n","    batches = 0\n","    start = time.time()\n","    for batch in loader:\n","        if len(batch) == 0:\n","            continue\n","        images, targets = batch\n","        images = [img.to(device) for img in images]\n","        targets = [{k: v.to(device) for k,v in t.items()} for t in targets]\n","\n","        loss_dict = model(images, targets)\n","        losses = sum(loss for loss in loss_dict.values())\n","\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","\n","        batch_loss = float(losses.item())\n","        epoch_loss += batch_loss\n","        batches += 1\n","        global_step += 1\n","        wandb.log({\"batch_loss\": batch_loss, \"epoch\": epoch, \"step\": global_step})\n","\n","    avg_loss = epoch_loss / batches if batches>0 else epoch_loss\n","    print(f\"Epoch {epoch}/{EPOCHS} — avg_loss: {avg_loss:.4f} — time: {time.time()-start:.1f}s\")\n","    wandb.log({\"epoch_loss\": avg_loss, \"epoch\": epoch})\n","\n","    if avg_loss < best_loss:\n","        best_loss = avg_loss\n","        ckpt_path = \"best_overfit_50.pth\"\n","        torch.save(model.state_dict(), ckpt_path)\n","        wandb.save(ckpt_path)\n","        print(\"  Saved best checkpoint:\", ckpt_path)\n","\n","    if epoch % 5 == 0 or epoch == EPOCHS:\n","        model.eval()\n","        for data in loader:\n","            if len(data) == 0:\n","                continue\n","            imgs, tgts = data\n","            break\n","        with torch.no_grad():\n","            img0 = imgs[0]\n","            pred = model([img0.to(device)])[0]\n","        fig = make_vis_figure(img0, tgts[0], pred, conf_th=0.5)\n","        wandb.log({f\"viz_epoch_{epoch}\": wandb.Image(fig)})\n","        plt.close(fig)\n","        model.train()\n","\n","print(\"Training complete. Best avg loss:\", best_loss)\n","\n","# -------------------------\n","# POST-TRAINING: Visualize 15 images\n","# -------------------------\n","print(\"\\n\" + \"=\"*60)\n","print(\"Generating 15-image visualization grid after training...\")\n","print(\"=\"*60 + \"\\n\")\n","\n","model.eval()\n","\n","# Create a non-shuffled loader to get consistent results\n","vis_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2, collate_fn=collate_fn_filter)\n","\n","vis_images = []\n","vis_count = 0\n","max_vis = 15\n","\n","with torch.no_grad():\n","    for batch in vis_loader:\n","        if len(batch) == 0:\n","            continue\n","        if vis_count >= max_vis:\n","            break\n","\n","        imgs, tgts = batch\n","        img = imgs[0].to(device)\n","        tgt = tgts[0]\n","\n","        # Get prediction\n","        pred = model([img])[0]\n","\n","        # Create figure\n","        fig = make_vis_figure(img, tgt, pred, conf_th=0.5, img_idx=vis_count+1)\n","        vis_images.append(wandb.Image(fig))\n","\n","        # Also display in notebook\n","        plt.show()\n","        plt.close(fig)\n","\n","        vis_count += 1\n","        print(f\"Processed image {vis_count}/{max_vis}\")\n","\n","# Log all visualizations to W&B\n","wandb.log({\"final_predictions\": vis_images})\n","print(f\"\\n✓ Successfully visualized {vis_count} images\")\n","\n","# -------------------------\n","# Save final model\n","# -------------------------\n","wandb.finish()\n","torch.save(model.state_dict(), \"final_overfit_50.pth\")\n","print(\"Saved final_overfit_50.pth\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1xz1CF5pFo_Y2vFHdwfm5VQL-av1KiwoT"},"id":"rM4BiYXCaak1","executionInfo":{"status":"ok","timestamp":1762487734887,"user_tz":300,"elapsed":694709,"user":{"displayName":"Juliana Prada","userId":"17676931323159668000"}},"outputId":"46fbd3f4-2d76-4338-e402-0e05a4aca619"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"2zPTlM60aayS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QOA1voHcaLVD"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"13anmmdJUMAcaaVDd45MIMbZhGB_hTSal","timestamp":1762486891748}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}