{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"2X0au3kegkbL","executionInfo":{"status":"ok","timestamp":1764922767493,"user_tz":300,"elapsed":518867,"user":{"displayName":"Juliana Prada","userId":"17676931323159668000"}},"outputId":"395ed8d0-2233-4c2b-e0ad-00363fa30392"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n","  | |_| | '_ \\/ _` / _` |  _/ -_)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mizzyjuliana1\u001b[0m (\u001b[33mizzyjuliana1-university-of-florida\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Images path exists: True\n","Masks path exists : True\n","Num images: 7000\n","Num masks : 7000\n","Device: cuda\n","Mapping BDD IDs -> labels: {6: 1, 7: 2, 11: 3, 13: 4, 12: 5, 14: 6, 15: 7, 17: 8, 18: 9}\n","Dataset length after subset: 50\n","Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170M/170M [00:00<00:00, 202MB/s]\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.23.0"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20251205_081243-r7137trk</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/izzyjuliana1-university-of-florida/bdd100k-overfit-50-allclasses/runs/r7137trk' target=\"_blank\">overfit-50-allclasses-final-fix</a></strong> to <a href='https://wandb.ai/izzyjuliana1-university-of-florida/bdd100k-overfit-50-allclasses' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/izzyjuliana1-university-of-florida/bdd100k-overfit-50-allclasses' target=\"_blank\">https://wandb.ai/izzyjuliana1-university-of-florida/bdd100k-overfit-50-allclasses</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/izzyjuliana1-university-of-florida/bdd100k-overfit-50-allclasses/runs/r7137trk' target=\"_blank\">https://wandb.ai/izzyjuliana1-university-of-florida/bdd100k-overfit-50-allclasses/runs/r7137trk</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1/20 — avg_loss: 1.7462 — time: 36.8s\n","  Saved best checkpoint: best_overfit_50.pth\n","Epoch 2/20 — avg_loss: 1.2471 — time: 16.5s\n","  Saved best checkpoint: best_overfit_50.pth\n","Epoch 3/20 — avg_loss: 1.0639 — time: 17.3s\n","  Saved best checkpoint: best_overfit_50.pth\n","Epoch 4/20 — avg_loss: 0.8846 — time: 18.3s\n","  Saved best checkpoint: best_overfit_50.pth\n","Epoch 5/20 — avg_loss: 0.7603 — time: 18.0s\n","  Saved best checkpoint: best_overfit_50.pth\n","Epoch 6/20 — avg_loss: 0.6715 — time: 18.4s\n","  Saved best checkpoint: best_overfit_50.pth\n","Epoch 7/20 — avg_loss: 0.5825 — time: 19.0s\n","  Saved best checkpoint: best_overfit_50.pth\n","Epoch 8/20 — avg_loss: 0.5330 — time: 18.2s\n","  Saved best checkpoint: best_overfit_50.pth\n","Epoch 9/20 — avg_loss: 0.5221 — time: 18.5s\n","  Saved best checkpoint: best_overfit_50.pth\n","Epoch 10/20 — avg_loss: 0.4669 — time: 18.2s\n","  Saved best checkpoint: best_overfit_50.pth\n","Epoch 11/20 — avg_loss: 0.4711 — time: 18.3s\n","Epoch 12/20 — avg_loss: 0.4301 — time: 18.5s\n","  Saved best checkpoint: best_overfit_50.pth\n","Epoch 13/20 — avg_loss: 0.4087 — time: 18.2s\n","  Saved best checkpoint: best_overfit_50.pth\n","Epoch 14/20 — avg_loss: 0.3677 — time: 18.2s\n","  Saved best checkpoint: best_overfit_50.pth\n","Epoch 15/20 — avg_loss: 0.3515 — time: 19.0s\n","  Saved best checkpoint: best_overfit_50.pth\n","Epoch 16/20 — avg_loss: 0.3567 — time: 18.1s\n","Epoch 17/20 — avg_loss: 0.3531 — time: 18.1s\n","Epoch 18/20 — avg_loss: 0.3643 — time: 18.2s\n","Epoch 19/20 — avg_loss: 0.3303 — time: 18.0s\n","  Saved best checkpoint: best_overfit_50.pth\n","Epoch 20/20 — avg_loss: 0.3146 — time: 18.7s\n","  Saved best checkpoint: best_overfit_50.pth\n","Training complete. Best avg loss: 0.31464239954948425\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>▇█▆▃▄▅▄▄▄▃▄▃▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▁▂▂▁▁▂▁▁▂▂▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇███</td></tr><tr><td>epoch_loss</td><td>█▆▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>0.32463</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>epoch_loss</td><td>0.31464</td></tr><tr><td>step</td><td>500</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">overfit-50-allclasses-final-fix</strong> at: <a href='https://wandb.ai/izzyjuliana1-university-of-florida/bdd100k-overfit-50-allclasses/runs/r7137trk' target=\"_blank\">https://wandb.ai/izzyjuliana1-university-of-florida/bdd100k-overfit-50-allclasses/runs/r7137trk</a><br> View project at: <a href='https://wandb.ai/izzyjuliana1-university-of-florida/bdd100k-overfit-50-allclasses' target=\"_blank\">https://wandb.ai/izzyjuliana1-university-of-florida/bdd100k-overfit-50-allclasses</a><br>Synced 5 W&B file(s), 4 media file(s), 0 artifact file(s) and 1 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20251205_081243-r7137trk/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saved final_overfit_50.pth\n"]}],"source":["# =========================\n","# Overfit 50 images — Mask R-CNN (BDD classes kept separate) + W&B logging\n","# The core logic is correct: ONLY BDD_IDS (Things) are processed.\n","# This code version includes explicit mask loading (Image.convert('L')) and max_area_ratio checks.\n","# =========================\n","!pip install wandb -q\n","import wandb\n","wandb.login()\n","\n","import os\n","import time\n","import torch\n","import numpy as np\n","from PIL import Image\n","import torchvision\n","import torchvision.transforms as T\n","from torch.utils.data import Dataset, DataLoader\n","from scipy.ndimage import label as cc_label\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from google.colab import drive\n","\n","# -------------------------\n","# Paths (your Drive layout)\n","# -------------------------\n","drive.mount('/content/drive', force_remount=True)\n","\n","BDD_ROOT = \"/content/drive/MyDrive/Latitude_AI_Team/data\"\n","IMG_DIR = f\"{BDD_ROOT}/bdd100k_images_10k/10k/train\"\n","MASK_DIR = f\"{BDD_ROOT}/bdd100k_segmentations/bdd100k_seg_maps_2/labels/train\"\n","\n","print(\"Images path exists:\", os.path.exists(IMG_DIR))\n","print(\"Masks path exists :\", os.path.exists(MASK_DIR))\n","print(\"Num images:\", len(os.listdir(IMG_DIR)))\n","print(\"Num masks :\", len(os.listdir(MASK_DIR)))\n","\n","# -------------------------\n","# Device + Hyperparams\n","# -------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", device)\n","\n","SUBSET = 50\n","EPOCHS = 20\n","BATCH_SIZE = 2\n","LR = 1e-4\n","\n","# *** CRITICAL: ONLY BDD INSTANCE (THING) CLASSES ARE LISTED HERE ***\n","# This guarantees that IDs like 8 (Vegetation/Tree) and 10 (Sky) are ignored.\n","BDD_IDS = [\n","    6,  # Traffic Light\n","    7,  # Traffic Sign\n","    11, # Person\n","    13, # Car\n","    12, # Rider\n","    14, # Truck\n","    15, # Bus\n","    17, # Motorcycle\n","    18  # Bicycle\n","]\n","NUM_FOREGROUND = len(BDD_IDS)\n","NUM_CLASSES = 1 + NUM_FOREGROUND\n","\n","ID_TO_LABEL = {bdd_id: i+1 for i, bdd_id in enumerate(BDD_IDS)}\n","print(\"Mapping BDD IDs -> labels:\", ID_TO_LABEL)\n","\n","# -------------------------\n","# Dataset (semantic -> instances)\n","# -------------------------\n","class BDDOverfitDataset(Dataset):\n","    def __init__(self, img_dir, mask_dir, subset=50, transforms=None, min_area=200, max_area_ratio=0.5):\n","        self.img_dir = img_dir\n","        self.mask_dir = mask_dir\n","        self.transforms = transforms\n","        self.min_area = min_area\n","        self.max_area_ratio = max_area_ratio\n","\n","        imgs = sorted([f for f in os.listdir(img_dir) if f.lower().endswith(('.jpg', '.png'))])\n","        masks = sorted([f for f in os.listdir(mask_dir) if f.lower().endswith(('.png', '.jpg'))])\n","\n","        # Align by basename (This logic is CONFIRMED correct)\n","        img_map = {os.path.splitext(f)[0]: f for f in imgs}\n","        mask_map = {os.path.splitext(f)[0].replace(\"_train_id\", \"\"): f for f in masks}\n","\n","        common = sorted(set(img_map.keys()).intersection(set(mask_map.keys())))\n","        if len(common) == 0:\n","            raise RuntimeError(\"No matching image/mask basenames found. Check directories and filenames.\")\n","\n","        # Keep only subset\n","        common = common[:subset]\n","        self.images = [img_map[k] for k in common]\n","        self.masks = [mask_map[k] for k in common]\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.images[idx])\n","        mask_path = os.path.join(self.mask_dir, self.masks[idx])\n","\n","        img = Image.open(img_path).convert(\"RGB\")\n","\n","        # *** CONFIRMED FIX: Explicitly load mask as single channel (L) ***\n","        mask_pil = Image.open(mask_path).convert('L')\n","        mask_np = np.array(mask_pil)\n","\n","        # Calculate max allowed area based on image dimensions\n","        H, W = mask_np.shape\n","        max_allowed_area = H * W * self.max_area_ratio\n","\n","        instance_masks = []\n","        labels = []\n","        boxes = []\n","\n","        # This loop only iterates over the desired THING IDs\n","        for bdd_id in BDD_IDS:\n","\n","            class_mask = (mask_np == bdd_id).astype(np.uint8)\n","\n","            if class_mask.sum() == 0:\n","                continue\n","\n","            inst_map, ninst = cc_label(class_mask)\n","            for inst_id in range(1, ninst + 1):\n","                inst_mask = (inst_map == inst_id)\n","                current_area = inst_mask.sum()\n","\n","                # Filter 1: Min Area\n","                if current_area < self.min_area:\n","                    continue\n","\n","                # Filter 2: Max Area (Robustness check)\n","                if current_area > max_allowed_area:\n","                    continue\n","\n","                ys, xs = np.where(inst_mask)\n","                xmin, ymin, xmax, ymax = xs.min(), ys.min(), xs.max(), ys.max()\n","                instance_masks.append(inst_mask.astype(np.uint8))\n","                labels.append(ID_TO_LABEL[bdd_id])\n","                boxes.append([xmin, ymin, xmax, ymax])\n","\n","        if len(instance_masks) == 0:\n","            return None\n","\n","        masks_tensor = torch.as_tensor(np.stack(instance_masks), dtype=torch.uint8)\n","        labels_tensor = torch.as_tensor(labels, dtype=torch.int64)\n","        boxes_tensor = torch.as_tensor(boxes, dtype=torch.float32)\n","        image_tensor = T.ToTensor()(img)\n","\n","        target = {\n","            \"boxes\": boxes_tensor,\n","            \"labels\": labels_tensor,\n","            \"masks\": masks_tensor,\n","            \"image_id\": torch.tensor([idx], dtype=torch.int64)\n","        }\n","\n","        if self.transforms:\n","            image_tensor = self.transforms(image_tensor)\n","\n","        return image_tensor, target\n","\n","# collate function (CONFIRMED correct)\n","def collate_fn_filter(batch):\n","    batch = [b for b in batch if b is not None]\n","    if len(batch) == 0:\n","        return tuple()\n","    return tuple(zip(*batch))\n","\n","# -------------------------\n","# Dataset + loader\n","# -------------------------\n","dataset = BDDOverfitDataset(IMG_DIR, MASK_DIR, subset=SUBSET, transforms=None, min_area=200, max_area_ratio=0.5)\n","loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=collate_fn_filter)\n","print(\"Dataset length after subset:\", len(dataset))\n","\n","# -------------------------\n","# Model\n","# -------------------------\n","def get_model(num_classes):\n","    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n","    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","    model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(in_features_mask, 256, num_classes)\n","    return model\n","\n","model = get_model(NUM_CLASSES).to(device)\n","optimizer = torch.optim.Adam([p for p in model.parameters() if p.requires_grad], lr=LR)\n","\n","# -------------------------\n","# W&B init\n","# -------------------------\n","wandb.init(project=\"bdd100k-overfit-50-allclasses\", name=\"overfit-50-allclasses-final-fix\", config={\n","    \"subset\": SUBSET, \"epochs\": EPOCHS, \"batch_size\": BATCH_SIZE, \"lr\": LR, \"num_classes\": NUM_CLASSES\n","})\n","\n","# -------------------------\n","# Visualization helper\n","# -------------------------\n","def make_vis_figure(img_tensor, target, prediction=None, conf_th=0.5):\n","    img_np = (img_tensor.permute(1,2,0).cpu().numpy() * 255).astype(np.uint8)\n","    fig, ax = plt.subplots(1,1, figsize=(10,6))\n","    ax.imshow(img_np)\n","    ax.axis('off')\n","\n","    # GT masks in green\n","    if target is not None:\n","        masks = target[\"masks\"].cpu().numpy()\n","        boxes = target[\"boxes\"].cpu().numpy()\n","        labels = target[\"labels\"].cpu().numpy()\n","        for k in range(masks.shape[0]):\n","            mask = masks[k]\n","            ax.imshow(np.ma.masked_where(mask==0, mask), cmap='Greens', alpha=0.4)\n","            x1,y1,x2,y2 = boxes[k]\n","            ax.add_patch(patches.Rectangle((x1,y1), x2-x1, y2-y1, edgecolor='lime', fill=False, linewidth=2))\n","            ax.text(x1, y1-5, f\"GT:{labels[k]}\", color='lime', fontsize=8, weight='bold')\n","\n","    # Prediction masks in red\n","    if prediction is not None:\n","        pred_masks = prediction.get(\"masks\", None)\n","        pred_boxes = prediction.get(\"boxes\", None)\n","        pred_scores = prediction.get(\"scores\", torch.tensor([])).cpu().numpy()\n","        keep = pred_scores >= conf_th\n","\n","        if pred_masks is not None and keep.sum() > 0:\n","            pred_masks = pred_masks.cpu().numpy()[keep,0]\n","            for mask in pred_masks:\n","                ax.imshow(np.ma.masked_where(mask<0.5, mask), cmap='Reds', alpha=0.4)\n","\n","        if pred_boxes is not None and keep.sum() > 0:\n","            pred_boxes = pred_boxes.cpu().numpy()[keep]\n","            for b in pred_boxes:\n","                x1,y1,x2,y2 = b\n","                ax.add_patch(patches.Rectangle((x1,y1), x2-x1, y2-y1, edgecolor='red', fill=False, linewidth=2))\n","\n","    return fig\n","\n","# -------------------------\n","# Training loop\n","# -------------------------\n","best_loss = float('inf')\n","global_step = 0\n","model.train()\n","\n","for epoch in range(1, EPOCHS+1):\n","    epoch_loss = 0.0\n","    batches = 0\n","    start = time.time()\n","    for batch in loader:\n","        if len(batch) == 0:\n","            continue\n","        images, targets = batch\n","        images = [img.to(device) for img in images]\n","        targets = [{k: v.to(device) for k,v in t.items()} for t in targets]\n","\n","        loss_dict = model(images, targets)\n","        losses = sum(loss for loss in loss_dict.values())\n","\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","\n","        batch_loss = float(losses.item())\n","        epoch_loss += batch_loss\n","        batches += 1\n","        global_step += 1\n","        wandb.log({\"batch_loss\": batch_loss, \"epoch\": epoch, \"step\": global_step})\n","\n","    avg_loss = epoch_loss / batches if batches>0 else epoch_loss\n","    print(f\"Epoch {epoch}/{EPOCHS} — avg_loss: {avg_loss:.4f} — time: {time.time()-start:.1f}s\")\n","    wandb.log({\"epoch_loss\": avg_loss, \"epoch\": epoch})\n","\n","    # Save checkpoint if improved\n","    if avg_loss < best_loss:\n","        best_loss = avg_loss\n","        ckpt_path = \"best_overfit_50.pth\"\n","        torch.save(model.state_dict(), ckpt_path)\n","        wandb.save(ckpt_path)\n","        print(\"  Saved best checkpoint:\", ckpt_path)\n","\n","    # Visualization\n","    if epoch % 5 == 0 or epoch == EPOCHS:\n","        model.eval()\n","        for data in loader:\n","            if len(data) == 0:\n","                continue\n","            imgs, tgts = data\n","            break\n","        with torch.no_grad():\n","            img0 = imgs[0]\n","            pred = model([img0.to(device)])[0]\n","        fig = make_vis_figure(img0, tgts[0], pred, conf_th=0.5)\n","        wandb.log({f\"viz_epoch_{epoch}\": wandb.Image(fig)})\n","        plt.close(fig)\n","        model.train()\n","\n","print(\"Training complete. Best avg loss:\", best_loss)\n","wandb.finish()\n","torch.save(model.state_dict(), \"final_overfit_50.pth\")\n","print(\"Saved final_overfit_50.pth\")"]},{"cell_type":"code","source":[],"metadata":{"id":"D7WG_6cnIX7u"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}