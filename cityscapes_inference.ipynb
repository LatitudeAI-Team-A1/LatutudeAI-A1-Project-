{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyO8UoKHNGHyVW+M0Fkz9yUY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"iZ4J7Diy65wH"},"outputs":[],"source":["\"\"\"\n","This code runs inference on the Cityscapes dataset using our best bdd100k model\n","\"\"\""]},{"cell_type":"code","source":["# ==============================================================================\n","# CELL 1: SETUP & PATHS\n","# ==============================================================================\n","import os\n","import torch\n","import torchvision\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from PIL import Image\n","from google.colab import drive\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from glob import glob\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","\n","# 1. Mount Drive\n","if not os.path.exists('/content/drive'):\n","    print(\"Mounting Google Drive...\")\n","    drive.mount('/content/drive')\n","\n","# 2. Configuration\n","DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n","print(f\"Running Inference on: {DEVICE}\")\n","\n","# Paths (Adjust if your folder structure is different)\n","DRIVE_ROOT = \"/content/drive/MyDrive/Latitude_AI_Team/data\"\n","CITYSCAPES_ROOT = os.path.join(DRIVE_ROOT, \"cityscapes\")\n","CITYSCAPES_IMG_DIR = os.path.join(CITYSCAPES_ROOT, 'leftImg8bit', 'train')\n","CITYSCAPES_LABEL_DIR = os.path.join(CITYSCAPES_ROOT, 'gtFine', 'train')\n","\n","# Path to your BEST BDD100K Model\n","MODEL_PATH = \"/content/drive/MyDrive/Latitude_AI_Team/models/best_model_checkpoint_full_run.pth\"\n","\n","# Verify paths\n","if not os.path.exists(CITYSCAPES_IMG_DIR):\n","    print(f\" Cityscapes Images not found at: {CITYSCAPES_IMG_DIR}\")\n","else:\n","    print(f\" Cityscapes Images found.\")\n","\n","if not os.path.exists(MODEL_PATH):\n","    print(f\" Model Checkpoint not found at: {MODEL_PATH}\")\n","else:\n","    print(f\"Model Checkpoint found.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HRHoL8z47Gou","executionInfo":{"status":"ok","timestamp":1764727118442,"user_tz":300,"elapsed":68559,"user":{"displayName":"Joseph Boadi","userId":"06232502213247757960"}},"outputId":"ce7ea878-a65d-4ad1-c64e-c9c90e42a219"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ”Œ Mounting Google Drive...\n","Mounted at /content/drive\n","ðŸš€ Running Inference on: cuda\n"," Cityscapes Images found.\n","Model Checkpoint found.\n"]}]},{"cell_type":"code","source":["# DATASET & MODEL DEFINITIONS\n","\n","# CITYSCAPES DATASET CLASS\n","class CityscapesDataset(Dataset):\n","    def __init__(self, img_dir, label_dir, transforms=None, limit=None):\n","        self.img_dir = img_dir\n","        self.label_dir = label_dir\n","        self.transforms = transforms\n","\n","        # Recursive search for files (Cityscapes has subfolders like 'aachen', 'bochum')\n","        self.image_paths = sorted(glob(os.path.join(img_dir, '**', '*leftImg8bit.png'), recursive=True))\n","        self.label_paths = sorted(glob(os.path.join(label_dir, '**', '*gtFine_instanceIds.png'), recursive=True))\n","\n","        if limit:\n","            self.image_paths = self.image_paths[:limit]\n","            self.label_paths = self.label_paths[:limit]\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n","\n","        # Load instance IDs (Ground Truth for visualization)\n","        instance_ids_img = Image.open(self.label_paths[idx])\n","        instance_ids_np = np.array(instance_ids_img, dtype=np.int32)\n","\n","        obj_ids = np.unique(instance_ids_np)\n","        # Cityscapes IDs < 10000 are usually groups/background\n","        obj_ids = obj_ids[obj_ids >= 1000]\n","\n","        masks = []\n","        boxes = []\n","        labels = []\n","\n","        for obj_id in obj_ids:\n","            mask_instance = (instance_ids_np == obj_id)\n","            if mask_instance.sum() < 10: continue # Filter noise\n","\n","            pos = np.where(mask_instance)\n","            xmin = np.min(pos[1]); xmax = np.max(pos[1])\n","            ymin = np.min(pos[0]); ymax = np.max(pos[0])\n","\n","            if (xmax - xmin) < 5 or (ymax - ymin) < 5: continue\n","\n","            boxes.append([xmin, ymin, xmax, ymax])\n","            masks.append(mask_instance)\n","            labels.append(1) # Placeholder label (We only care about visual overlap for now)\n","\n","        if boxes:\n","            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","            labels = torch.as_tensor(labels, dtype=torch.int64)\n","            masks = torch.as_tensor(np.stack(masks), dtype=torch.uint8)\n","        else:\n","            H, W = instance_ids_np.shape\n","            boxes = torch.zeros((0, 4), dtype=torch.float32)\n","            labels = torch.zeros((0,), dtype=torch.int64)\n","            masks = torch.zeros((0, H, W), dtype=torch.uint8)\n","\n","        target = {\"boxes\": boxes, \"labels\": labels, \"masks\": masks, \"image_id\": torch.tensor([idx])}\n","\n","        if self.transforms:\n","            img = self.transforms(img)\n","\n","        return img, target\n","\n","# --- 2. MODEL DEFINITION (Must match BDD Training) ---\n","BDD_IDS = [6, 7, 11, 13, 12, 14, 15, 17, 18]\n","NUM_CLASSES = 1 + len(BDD_IDS) # Background + 9 classes\n","\n","def get_model(num_classes):\n","    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=None) # No pre-train needed, we load ours\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, 256, num_classes)\n","    return model\n","\n","# --- 3. LOADERS ---\n","def get_cityscapes_dataloader(limit=5):\n","    transform = transforms.Compose([transforms.ToTensor()])\n","    dataset = CityscapesDataset(CITYSCAPES_IMG_DIR, CITYSCAPES_LABEL_DIR, transforms=transform, limit=limit)\n","    return DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"],"metadata":{"id":"McgqtPwf7bsG","executionInfo":{"status":"ok","timestamp":1764727196892,"user_tz":300,"elapsed":18,"user":{"displayName":"Joseph Boadi","userId":"06232502213247757960"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["\n","\n","# 1. Load the Model with Trained Weights\n","print(f\" Loading model from: {MODEL_PATH}\")\n","model = get_model(NUM_CLASSES)\n","checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","model.to(DEVICE)\n","model.eval()\n","print(\"Model loaded successfully.\")\n","\n","# 2. Visualization Function\n","LABEL_TO_NAME = {\n","    1: \"Traffic Light\", 2: \"Traffic Sign\", 3: \"Person\", 4: \"Car\",\n","    5: \"Rider\", 6: \"Truck\", 7: \"Bus\", 8: \"Motorcycle\", 9: \"Bicycle\"\n","}\n","\n","def visualize_inference(image, prediction, conf_threshold=0.5):\n","    img_np = image.permute(1, 2, 0).cpu().numpy()\n","    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n","    ax.imshow(img_np)\n","    ax.set_title(\"Cityscapes Inference (BDD Model)\", fontsize=16)\n","    ax.axis('off')\n","\n","    # Prediction data\n","    boxes = prediction[0]['boxes'].cpu().numpy()\n","    labels = prediction[0]['labels'].cpu().numpy()\n","    scores = prediction[0]['scores'].cpu().numpy()\n","    masks = prediction[0]['masks'].cpu().numpy()\n","\n","    for i, score in enumerate(scores):\n","        if score < conf_threshold: continue\n","\n","        # Draw Box (Red)\n","        x1, y1, x2, y2 = boxes[i]\n","        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='red', facecolor='none')\n","        ax.add_patch(rect)\n","\n","        # Draw Label\n","        label_name = LABEL_TO_NAME.get(labels[i], f\"ID {labels[i]}\")\n","        ax.text(x1, y1-5, f\"{label_name}: {score:.2f}\", color='red', fontsize=10, weight='bold', bbox=dict(facecolor='white', alpha=0.5))\n","\n","        # Draw Mask (Red overlay)\n","        mask = masks[i, 0]\n","        masked_image = np.ma.masked_where(mask < 0.5, mask)\n","        ax.imshow(masked_image, cmap='Reds', alpha=0.5)\n","\n","    plt.show()\n","\n","# 3. Run Loop on Cityscapes Data\n","test_loader = get_cityscapes_dataloader(limit=5) # Check first 5 images\n","\n","print(\" Starting Inference on Cityscapes Data...\")\n","with torch.no_grad():\n","    for i, (images, targets) in enumerate(test_loader):\n","        images = [img.to(DEVICE) for img in images]\n","\n","        # Run Inference\n","        predictions = model(images)\n","\n","        # Visualize\n","        visualize_inference(images[0], predictions, conf_threshold=0.5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Dvqz3FyzyfwB-KCxZbHlkbuEOulfYBT2"},"id":"Q9MucZma8AvG","executionInfo":{"status":"ok","timestamp":1764727386632,"user_tz":300,"elapsed":81323,"user":{"displayName":"Joseph Boadi","userId":"06232502213247757960"}},"outputId":"14608ee1-9b8e-458d-d31f-63c616b96b00"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["\n","# CELL 4: QUANTITATIVE EVALUATION ON CITYSCAPES (THE \"DOMAIN GAP\" CHECK)\n","!pip install torchmetrics\n","\n","\n","import torch\n","from torchmetrics.detection import MeanAveragePrecision\n","from tqdm import tqdm\n","\n","\n","\n","CITYSCAPES_MAPPING = {\n","    # Cityscapes Train ID : BDD Model ID\n","    26: 4, # Car\n","    24: 3, # Person\n","    25: 5, # Rider\n","    27: 6, # Truck\n","    28: 7, # Bus\n","    32: 8, # Motorcycle\n","    33: 9, # Bicycle\n","    7: 1,  # Traffic Light (based on standard Cityscapes ID)\n","    8: 2   # Traffic Sign\n","}\n","\n","\n","print(\"ðŸš€ Starting FINAL Evaluation (Class Agnostic Mode)...\")\n","# This ignores the specific label ID and just checks if BOXES and MASKS match.\n","metric = MeanAveragePrecision(box_format=\"xyxy\", iou_type=\"segm\", class_metrics=False).to(DEVICE)\n","\n","model.eval()\n","\n","with torch.no_grad():\n","    for i, (images, targets) in tqdm(enumerate(test_loader), total=len(test_loader)):\n","        images = [img.to(DEVICE) for img in images]\n","        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n","\n","        outputs = model(images)\n","\n","        processed_preds = []\n","        processed_targets = []\n","\n","        # Set all labels to '1' for both Preds and Targets\n","        # This forces the metric to ignore class mismatch and just check detection quality\n","        for pred in outputs:\n","            if 'masks' in pred:\n","                pred['masks'] = (pred['masks'] > 0.5).squeeze(1).to(torch.uint8)\n","            # FORCE LABEL 1\n","            pred['labels'] = torch.ones_like(pred['labels'])\n","            processed_preds.append(pred)\n","\n","        for tgt in targets:\n","            # FORCE LABEL 1\n","            tgt['labels'] = torch.ones_like(tgt['labels'])\n","            processed_targets.append(tgt)\n","\n","        metric.update(processed_preds, processed_targets)\n","\n","# 2. COMPUTE\n","print(\"\\nComputing Class-Agnostic Score (Pure Detection Quality)...\")\n","results = metric.compute()\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"CITYSCAPES ROBUSTNESS SCORE (Class Agnostic)\")\n","print(\"=\"*50)\n","print(f\"AP @[.50:.95]: {results['map'].item():.4f}\")\n","print(f\"AP @ .50:      {results['map_50'].item():.4f}\")\n","print(f\"AP @ .75:      {results['map_75'].item():.4f}\")\n","print(\"=\"*50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Old8L5p68bNP","executionInfo":{"status":"ok","timestamp":1764728939192,"user_tz":300,"elapsed":121794,"user":{"displayName":"Joseph Boadi","userId":"06232502213247757960"}},"outputId":"417901a5-4902-4ec9-d49a-8b4b33731314"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (1.8.2)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.9.0+cu126)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (0.15.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n","ðŸš€ Starting FINAL Evaluation (Class Agnostic Mode)...\n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:55<00:00,  1.16s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Computing Class-Agnostic Score (Pure Detection Quality)...\n","\n","==================================================\n","CITYSCAPES ROBUSTNESS SCORE (Class Agnostic)\n","==================================================\n","AP @[.50:.95]: 0.1034\n","AP @ .50:      0.2281\n","AP @ .75:      0.0828\n","==================================================\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"laOmGgIP_zQ_"},"execution_count":null,"outputs":[]}]}